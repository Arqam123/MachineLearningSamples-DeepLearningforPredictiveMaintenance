{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Operationalization & Deployment\n",
    "\n",
    "In the previous script, you learned how to save lstm trained models to files. You also learned that model weights are easily stored using  HDF5 format and that the network structure can be saved in JSON. In this script, you will learn how to load your models up, operationalize and use them to make future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# import the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import json\n",
    "import shutil\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, LSTM, Activation gdt\n",
    "\n",
    "import h5py\n",
    "from sklearn import datasets\n",
    "\n",
    "# For creating the deployment schema file\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "run_logger = get_azureml_logger()\n",
    "run_logger.log('amlrealworld.predictivemaintenanceforpm.operationalization','true')\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model storage \n",
    "\n",
    "We will stor the model in an Azure Blob Storage Container for easy retreival to your deployment platform. \n",
    "Instructions for setting up your Azure Storage account are available within this link (https://docs.microsoft.com/en-us/azure/storage/blobs/storage-python-how-to-use-blob-storage). You will need to copy your account name and account key from the _Access Keys_ area in the portal into the following code block. These credentials will be reused in all four Jupyter notebooks.\n",
    "\n",
    "We will handle creating the containers and writing the data to these containers for each notebook. Further instructions for using Azure Blob storage with AML Workbench are available (https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md).\n",
    "\n",
    "You will need to enter the **ACCOUNT_NAME** as well as the **ACCOUNT_KEY** in order to access Azure Blob storage account you have created. This notebook will create and store all the resulting data files in a blob container under this account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"pdmamlworkbench\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"o/FONpE8aFJ4olg156gRzW452fYEYQcFdc5nFiVPb49MSRiIS3I0maYSwsnVRXNnvZrhqKMZhupyhxH8jehqmg==\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the Data Ingestion and Preparation notebook is stored in the sensordata ingestion container.\n",
    "MODEL_CONTAINER = \"PM_LSTM_model\" \n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store each of these data sets in a local persistance folder\n",
    "SHARE_ROOT = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']\n",
    "\n",
    "# These file names detail the data files. \n",
    "TEST_DATA = 'PM_test_files.pkl'\n",
    "\n",
    "# We'll serialize the model in json format\n",
    "LSTM_MODEL = 'modellstm.json'\n",
    "\n",
    "# and store the weights in h5\n",
    "MODEL_WEIGHTS = 'modellstm.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data frame\n",
    "\n",
    "We haev previously stored the test data frame in the local persistance directory indicated by the _SHARE_ROOT_ variable. We'll use this data frame to test the model deployment and build the model schema to describe the deployment function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(SHARE_ROOT + TEST_DATA)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SHARE_ROOT + LSTM_MODEL, 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    print(\"json file read from shared folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SHARE_ROOT + MODEL_WEIGHTS, 'r') as model:\n",
    "    loaded_model.load_weights(os.path.join(SHARE_ROOT, MODEL_WEIGHTS))\n",
    "    print(\"model weights read from shared folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test init() and run() functions to read from the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    with open(SHARE_ROOT + LSTM_MODEL, 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    with open(SHARE_ROOT + MODEL_WEIGHTS, 'r') as model:\n",
    "        print(\"Loaded model\")\n",
    "    \n",
    "    #inputs_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"inputs\")\n",
    "    #print(inputs_dc)\n",
    "    #prediction_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the feature columns \n",
    "# pick the feature columns \n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "key_cols = ['id', 'cycle']\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "input_features = test_df.columns.values.tolist()\n",
    "sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "sensor_cols = [x for x in sensor_cols if x not in set(label_cols)]\n",
    "sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "# The time is sequenced along\n",
    "# This may be a silly way to get these column names, but it's relatively clear\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "print(sequence_cols)\n",
    "\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "seq_array_test_last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "label_array_test_last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_array_test_last.shape)\n",
    "print(label_array_test_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test metrics\n",
    "prediction = loaded_model.predict_proba(seq_array_test_last, verbose=2)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(score_df): \n",
    "    global clf2, inputs_dc, prediction_dc\n",
    "\n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "    label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "    input_features = test_df.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(label_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_df[score_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in score_df['id'].unique() if len(score_df[score_df['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        #inputs_dc.collect(seq_array)\n",
    "        #prediction_dc.collect(prediction)\n",
    "        return (prediction)\n",
    "    except Exception as e:\n",
    "        return(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model assets\n",
    "\n",
    "Next we persist the assets we have created to disk for use in operationalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"score_df\": SampleDefinition(DataTypes.PANDAS, \n",
    "                                       test_df[sequence_cols])}\n",
    "\n",
    "json_schema = generate_schema(run_func=run, inputs=inputs, filepath='service_schema.json')\n",
    "\n",
    "# save the schema file for deployment\n",
    "out = json.dumps(json_schema)\n",
    "with open(SHARE_ROOT + 'service_schema.json', 'w') as f:\n",
    "    f.write(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {SHARE_ROOT}/lstmscore.py\n",
    "\n",
    "# import the libraries\n",
    "import keras\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open('modellstm.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"modellstm.h5\")\n",
    "    print(\"Loaded model\")\n",
    "    \n",
    "    #inputs_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"inputs\")\n",
    "    #print(inputs_dc)\n",
    "    #prediction_dc = ModelDataCollector(\"modellstm.h5\", identifier=\"prediction\")\n",
    "    \n",
    "    \n",
    "def run(score_df): \n",
    "    global clf2, inputs_dc, prediction_dc\n",
    "\n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "    label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "    input_features = test_df.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(label_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_df[score_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in score_df['id'].unique() if len(score_df[score_df['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        #inputs_dc.collect(seq_array)\n",
    "        #prediction_dc.collect(prediction)\n",
    "        return (prediction)\n",
    "    except Exception as e:\n",
    "        return(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and generate the schema file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging\n",
    "\n",
    "To move the model artifacts around, we'll zip them into one file. We can then retreive this file from the persistance shared folder on your DSVM.\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the operationalization assets for easy blob storage transfer\n",
    "# We can remove the persisted data files.\n",
    "!rm {SHARE_ROOT}/PM*.pkl\n",
    "\n",
    "MODEL_O16N = shutil.make_archive('LSTM_o16n', 'zip', SHARE_ROOT)\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(MODEL_CONTAINER, \n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# Transfer the compressed operationalization assets into the blob container.\n",
    "az_blob_service.create_blob_from_path(MODEL_CONTAINER, \"LSTM_o16n.zip\", str(MODEL_O16N)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Once the assets are stored, we can download them into a local compute context for operationalization on an Azure web service.\n",
    "\n",
    "We demonstrate how to setup this web service this through a CLI window opened in the AML Workbench application. \n",
    "\n",
    "Once downloaded, unzip the file into the directory of your choosing. The zip file contains three deployment assets:\n",
    "\n",
    "- the `lstmscore.py` file\n",
    "- a `lstm.model` directory\n",
    "- the `modellstm.json` file\n",
    "\n",
    "\n",
    "\n",
    "## Create a model management endpoint \n",
    "\n",
    "Create a modelmanagement under your account. We will call this `lstmmodelmanagement`. The remaining defaults are acceptable.\n",
    "\n",
    "`az ml account modelmanagement create --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name lstmmodelmanagement`\n",
    "\n",
    "\n",
    "## Check environment settings\n",
    "\n",
    "Show what environment is currently active:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "If nothing is set, we setup the environment with the existing model management context first: \n",
    "\n",
    "` az ml env setup --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "then set the current environment:\n",
    "\n",
    "`az ml env set --resource-group <RESOURCE_GROUP> --cluster-name pdmmodelmanagement`\n",
    "\n",
    "Check that the environment is now set:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "\n",
    "## Deploy your web service \n",
    "\n",
    "Once the environment is setup, we'll deploy the web service from the CLI.\n",
    "\n",
    "These commands assume the current directory contains the webservice assets we created in throughout the notebooks in this scenario (`lstmscore.py`, `modellstm.json` and `lstm.model`). If your kernel has run locally, the assets will be in the `os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']`. \n",
    "\n",
    "On windows this points to:\n",
    "\n",
    "`\n",
    "cd C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name>\n",
    "`\n",
    "\n",
    "on linux variants this points to:\n",
    "\n",
    "`\n",
    "cd ~\\.azureml\\share\\<team account>\\<Project Name>\n",
    "`\n",
    "\n",
    "\n",
    "The command to create a web service (`<SERVICE_ID>`) with these operationalization assets in the current directory is:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f <filename> -r <TARGET_RUNTIME> -m <MODEL_FILE> -s <SCHEMA_FILE> -n <SERVICE_ID> --cpu 0.1\n",
    "`\n",
    "\n",
    "The default cluster has only 2 nodes with 2 cores each. Some cores are taken for system components. AMLWorkbench asks for 1 core per service. To deploy multiple services into this cluster, we specify the cpu requirement in the service create command as (--cpu 0.1) to request 10% of a core. \n",
    "\n",
    "For this example, we will call our webservice `amlworkbenchpdmwebservice`. This `SERVICE_ID` must be all lowercase, with no spaces:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f lstmscore.py -r spark-py -m lstm.model -s modellstm.json --cpu 0.1 -n amlworkbenchpdmwebservice\n",
    "`\n",
    "\n",
    "This command will take some time to execute. \n",
    "\n",
    "Once complete, the command returns sample usage commands to test the service for both PowerShell and the cmd prompt. We can execute these commands from the command line as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Once the assets are stored, we can download them into a deployment compute context for operationalization on an Azure web service. For this scenario, we will deploy this on our local docker container context.\n",
    "\n",
    "We demonstrate how to setup this web service this through a CLI window opened in the AML Workbench application. \n",
    "\n",
    "## Download the model\n",
    "\n",
    "To download the model we've saved, follow these instructions on a local computer.\n",
    "\n",
    "- the `lstmscore.py` file\n",
    "- the `modellstm.json` model file\n",
    "- the `modellstm.h5` weights file\n",
    "\n",
    "## Create a model management endpoint \n",
    "\n",
    "Create a modelmanagement under your account. We will call this `pdmmodelmanagement`. The remaining defaults are acceptable.\n",
    "\n",
    "`az ml account modelmanagement create --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "If you get a `ResourceGroupNotFound` error, you may need to set the correct subscription. This is typically only an issue if your Azure login connects to multiple subscritpions. \n",
    "\n",
    "`az account set -s '<subscription name>'`\n",
    "\n",
    "You can find the `subscription name` or `subscription id` through the (https://portal.azure.com) under the resource group you'd like to use.\n",
    "\n",
    "## Check environment settings\n",
    "\n",
    "Show what environment is currently active:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "If nothing is set, we setup the environment with the existing model management context first: \n",
    "\n",
    "` az ml env setup --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "using the same `<ACCOUNT_REGION>` and `<RESOURCE_GROUP>` in the previous section. Then set the current environment:\n",
    "\n",
    "`az ml env set --resource-group <RESOURCE_GROUP> --cluster-name pdmmodelmanagement`\n",
    "\n",
    "Check that the environment is now set:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "\n",
    "## Install docker to compute target\n",
    "\n",
    "Once the model management environment is setup, we'll deploy the web service from the CLI to a local docker container for this demonstration. This assumes you have docker installed localy (https://www.docker.com/get-docker).\n",
    "\n",
    "Once docker is installed and running, you will need to prepare the local docker container, just as we didi the remote container.\n",
    "\n",
    "`az ml experiment prepare -c docker`\n",
    "\n",
    "Now deploy the solution to this container.\n",
    "\n",
    "\n",
    "## Deploy a web service \n",
    "\n",
    "These commands assume the current directory contains the webservice assets we created in throughout the notebooks in this scenario (`lstmscore.py`, `modellstm.json` and `modellstm.h5`). Change to the directory where the zip file was unpacked. \n",
    "\n",
    "The command to create a web service (`<SERVICE_ID>`) with these operationalization assets in the current directory is:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f <filename> -r <TARGET_RUNTIME> -m <MODEL_FILE> -s <SCHEMA_FILE> -n <SERVICE_ID> --cpu 0.1\n",
    "`\n",
    "\n",
    "The default cluster has only 2 nodes with 2 cores each. Some cores are taken for system components. AMLWorkbench asks for 1 core per service. To deploy multiple services into this cluster, we specify the cpu requirement in the service create command as (--cpu 0.1) to request 10% of a core. \n",
    "\n",
    "For this example, we will call our webservice `amlworkbenchpdmwebservice`. This `SERVICE_ID` must be all lowercase, with no spaces:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f pdmscore.py -r spark-py -m pdmrfull.model -s service_schema.json --cpu 0.1 -n lstmwebservice\n",
    "`\n",
    "\n",
    "This command will take some time to execute. \n",
    "\n",
    "## Test your deployment.\n",
    "\n",
    "Once complete, the `az ml service create` command returns sample usage commands to test the service for both PowerShell and the cmd prompt. We can test this deployment by executing these commands from the command line. For our example:\n",
    "\n",
    "```\n",
    " az ml service run realtime -i amlworkbenchpdmwebservice --% -d \"{\\\"input_df\\\": [{\\\"rotate_rollingmean_36\\\": 450.0384342542265, \\\"rotate_rollingstd_36\\\": 0.0, \\\"volt_rollingmean_24\\\": 166.69782028530955, \\\"volt_rollingmean_36\\\": 166.5072079613422, \\\"comp1sum\\\": 504.0, \\\"comp2sum\\\": 564.0, \\\"error3sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingmean_24\\\": 40.302192663278625, \\\"pressure_rollingstd_24\\\": 0.0, \\\"vibration_rollingstd_12\\\": 0.0, \\\"pressure_rollingstd_12\\\": 0.0, \\\"rotate_rollingmean_12\\\": 445.7130438343768, \\\"volt_rollingstd_24\\\": 0.0, \\\"comp3sum\\\": 444.0, \\\"error1sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingstd_36\\\": 0.0, \\\"rotate_rollingstd_24\\\": 0.0, \\\"volt_rollingstd_36\\\": 0.0, \\\"rotate_rollingmean_24\\\": 444.92430808877185, \\\"error2sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingstd_36\\\": 0.0, \\\"comp4sum\\\": 399.0, \\\"machineID\\\": 27, \\\"pressure_rollingmean_24\\\": 100.42784289855126, \\\"volt_rollingmean_12\\\": 162.37456132546583, \\\"error4sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingmean_12\\\": 103.46853199581041, \\\"age\\\": 9, \\\"pressure_rollingmean_36\\\": 99.1626730910439, \\\"volt_rollingstd_12\\\": 0.0, \\\"rotate_rollingstd_12\\\": 0.0, \\\"vibration_rollingmean_36\\\": 39.86004229336383, \\\"vibration_rollingstd_24\\\": 0.0, \\\"error5sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingmean_12\\\": 39.69610732198209}, {\\\"rotate_rollingmean_36\\\": 452.58602482190344, \\\"rotate_rollingstd_36\\\": 1.3063227195446807, \\\"volt_rollingmean_24\\\": 168.8315798036505, \\\"volt_rollingmean_36\\\": 166.8633264221902, \\\"comp1sum\\\": 504.0, \\\"comp2sum\\\": 564.0, \\\"error3sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingmean_24\\\": 39.8762193116053, \\\"pressure_rollingstd_24\\\": 0.5506261833397947, \\\"vibration_rollingstd_12\\\": 0.5581845837178677, \\\"pressure_rollingstd_12\\\": 1.3059590035299573, \\\"rotate_rollingmean_12\\\": 448.82482383859184, \\\"volt_rollingstd_24\\\": 1.1327450423992658, \\\"comp3sum\\\": 444.0, \\\"error1sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingstd_36\\\": 0.12802019423837702, \\\"rotate_rollingstd_24\\\": 6.2252625510326345, \\\"volt_rollingstd_36\\\": 1.2113288898088435, \\\"rotate_rollingmean_24\\\": 455.68853459771736, \\\"error2sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingstd_36\\\": 0.360813923769749, \\\"comp4sum\\\": 399.0, \\\"machineID\\\": 27, \\\"pressure_rollingmean_24\\\": 98.84197839575184, \\\"volt_rollingmean_12\\\": 169.6342364499553, \\\"error4sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingmean_12\\\": 100.13428527324218, \\\"age\\\": 9, \\\"pressure_rollingmean_36\\\": 99.18126302139088, \\\"volt_rollingstd_12\\\": 1.7162303092954838, \\\"rotate_rollingstd_12\\\": 7.358009183124642, \\\"vibration_rollingmean_36\\\": 39.83194043387068, \\\"vibration_rollingstd_24\\\": 0.26866456414969686, \\\"error5sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingmean_12\\\": 40.534215611846555}, {\\\"rotate_rollingmean_36\\\": 452.6366978657443, \\\"rotate_rollingstd_36\\\": 0.726203655443797, \\\"volt_rollingmean_24\\\": 165.47787140830766, \\\"volt_rollingmean_36\\\": 164.9839282666808, \\\"comp1sum\\\": 503.0, \\\"comp2sum\\\": 563.0, \\\"error3sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingmean_24\\\": 39.48080284488274, \\\"pressure_rollingstd_24\\\": 0.43573594568766316, \\\"vibration_rollingstd_12\\\": 0.33150005427630586, \\\"pressure_rollingstd_12\\\": 0.30398746497620055, \\\"rotate_rollingmean_12\\\": 462.5522453568429, \\\"volt_rollingstd_24\\\": 1.388783538126311, \\\"comp3sum\\\": 443.0, \\\"error1sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingstd_36\\\": 0.06733738203927228, \\\"rotate_rollingstd_24\\\": 2.2615583783043336, \\\"volt_rollingstd_36\\\": 0.4066137169118576, \\\"rotate_rollingmean_24\\\": 454.4666253135592, \\\"error2sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingstd_36\\\": 0.40800640702349306, \\\"comp4sum\\\": 398.0, \\\"machineID\\\": 27, \\\"pressure_rollingmean_24\\\": 98.70475189546528, \\\"volt_rollingmean_12\\\": 168.0289231573457, \\\"error4sum_rollingmean_24\\\": 0.0, \\\"pressure_rollingmean_12\\\": 97.5496715182615, \\\"age\\\": 9, \\\"pressure_rollingmean_36\\\": 99.92595364177775, \\\"volt_rollingstd_12\\\": 1.9026812928919759, \\\"rotate_rollingstd_12\\\": 12.545522310840685, \\\"vibration_rollingmean_36\\\": 39.16084871098736, \\\"vibration_rollingstd_24\\\": 0.2757178837764945, \\\"error5sum_rollingmean_24\\\": 0.0, \\\"vibration_rollingmean_12\\\": 39.21822301136402}]}\"\n",
    "```\n",
    "This submits 3 records to the model through the web service, and returns predictioned output labels for each of the three rows:\n",
    "\n",
    "```\n",
    "\"0.0,0.0,0.0\"\n",
    "```\n",
    "\n",
    "Indicating that these records are predicted to be healthy with in the requested 7 day time window.\n",
    "\n",
    "\n",
    "We can view additional service usage information with the following command. \n",
    "\n",
    "```az ml service usage realtime -i amlworkbenchpdmwebservice```\n",
    "\n",
    "Which indicates how the service is currently deployed:\n",
    "\n",
    "```\n",
    "Scoring URL:\n",
    "    http://127.0.0.1:32770/score\n",
    "\n",
    "Headers:\n",
    "    Content-Type: application/json\n",
    "\n",
    "Swagger URL:\n",
    "    http://127.0.0.1:32770/swagger.json\n",
    "\n",
    "Sample CLI command:\n",
    "...\n",
    "```\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Working through all of these notebooks, we have completed:\n",
    "\n",
    " * Data aquisition in `Code/1_data_aquisition.ipynb` notebook.\n",
    " * Time series feature engineering and failure labeling to predict component failures within a 7 day window in the `Code/2_feature_engineering.ipynb` notebook.\n",
    " * Model building and evaluation in the `Code/3_model_building.ipynb` notebook.\n",
    " * Deployment asset generation and model deployment in the `Code/4_operationalization.ipynb` notebook.\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "LSTM DSVM",
   "language": "python",
   "name": "lstm_dsvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
