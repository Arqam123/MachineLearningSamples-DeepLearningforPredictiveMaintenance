{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model Operationalization & Deployment\n",
    "\n",
    "In the previous script, you learned how to save lstm trained models to files. You also learned that model weights are easily stored using  HDF5 format and that the network structure can be saved in JSON. In this script, you will learn how to load your models up, operationalize and use them to make future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# import the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import h5py\n",
    "\n",
    "# For creating the deployment schema file\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "run_logger = get_azureml_logger()\n",
    "run_logger.log('amlrealworld.predictivemaintenanceforpm.operationalization','true')\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model storage \n",
    "\n",
    "We will stor the model in an Azure Blob Storage Container for easy retreival to your deployment platform. \n",
    "Instructions for setting up your Azure Storage account are available within this link (https://docs.microsoft.com/en-us/azure/storage/blobs/storage-python-how-to-use-blob-storage). You will need to copy your account name and account key from the _Access Keys_ area in the portal into the following code block. These credentials will be reused in all four Jupyter notebooks.\n",
    "\n",
    "We will handle creating the containers and writing the data to these containers for each notebook. Further instructions for using Azure Blob storage with AML Workbench are available (https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md).\n",
    "\n",
    "You will need to enter the **ACCOUNT_NAME** as well as the **ACCOUNT_KEY** in order to access Azure Blob storage account you have created. This notebook will create and store all the resulting data files in a blob container under this account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the Data Ingestion and Preparation notebook is stored in the sensordata ingestion container.\n",
    "MODEL_CONTAINER = \"pmlstmmodel\" \n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the environment required to build the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will store each of these data sets in a local persistance folder\n",
    "SHARE_ROOT = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']\n",
    "\n",
    "# These file names detail the data files. \n",
    "TEST_DATA = 'PM_test_files.pkl'\n",
    "\n",
    "# We'll serialize the model in json format\n",
    "LSTM_MODEL = 'modellstm.json'\n",
    "\n",
    "# and store the weights in h5\n",
    "MODEL_WEIGHTS = 'modellstm.h5'\n",
    "\n",
    "# and the schema file\n",
    "SCHEMA_FILE = 'service_schema.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data frame\n",
    "\n",
    "We have previously stored the test data frame in the local persistance directory indicated by the SHARE_ROOT variable. We'll use this data frame to test the model deployment and build the model schema to describe the deployment function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cycle</th>\n",
       "      <th>setting1</th>\n",
       "      <th>setting2</th>\n",
       "      <th>setting3</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>...</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>cycle_norm</th>\n",
       "      <th>RUL</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545181</td>\n",
       "      <td>0.310661</td>\n",
       "      <td>0.269413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.661834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150602</td>\n",
       "      <td>0.379551</td>\n",
       "      <td>0.222316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.682171</td>\n",
       "      <td>0.686827</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376506</td>\n",
       "      <td>0.346632</td>\n",
       "      <td>0.322248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.721348</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370482</td>\n",
       "      <td>0.285154</td>\n",
       "      <td>0.408001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.662110</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391566</td>\n",
       "      <td>0.352082</td>\n",
       "      <td>0.332039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658915</td>\n",
       "      <td>0.716377</td>\n",
       "      <td>0.011080</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271084</td>\n",
       "      <td>0.176150</td>\n",
       "      <td>0.217421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.596899</td>\n",
       "      <td>0.624827</td>\n",
       "      <td>0.013850</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271084</td>\n",
       "      <td>0.268149</td>\n",
       "      <td>0.381330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550388</td>\n",
       "      <td>0.691798</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400602</td>\n",
       "      <td>0.214737</td>\n",
       "      <td>0.314652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.705426</td>\n",
       "      <td>0.591273</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201807</td>\n",
       "      <td>0.485066</td>\n",
       "      <td>0.506921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.770367</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.259036</td>\n",
       "      <td>0.309789</td>\n",
       "      <td>0.276671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.565891</td>\n",
       "      <td>0.673571</td>\n",
       "      <td>0.024931</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n",
       "0   1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661  0.269413   \n",
       "1   1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551  0.222316   \n",
       "2   1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632  0.322248   \n",
       "3   1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154  0.408001   \n",
       "4   1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082  0.332039   \n",
       "5   1      6  0.568966  0.750000       0.0  0.0  0.271084  0.176150  0.217421   \n",
       "6   1      7  0.500000  0.666667       0.0  0.0  0.271084  0.268149  0.381330   \n",
       "7   1      8  0.534483  0.500000       0.0  0.0  0.400602  0.214737  0.314652   \n",
       "8   1      9  0.293103  0.500000       0.0  0.0  0.201807  0.485066  0.506921   \n",
       "9   1     10  0.356322  0.416667       0.0  0.0  0.259036  0.309789  0.276671   \n",
       "\n",
       "    s5   ...    s16       s17  s18  s19       s20       s21  cycle_norm  RUL  \\\n",
       "0  0.0   ...    0.0  0.333333  0.0  0.0  0.558140  0.661834    0.000000  142   \n",
       "1  0.0   ...    0.0  0.416667  0.0  0.0  0.682171  0.686827    0.002770  141   \n",
       "2  0.0   ...    0.0  0.416667  0.0  0.0  0.728682  0.721348    0.005540  140   \n",
       "3  0.0   ...    0.0  0.250000  0.0  0.0  0.666667  0.662110    0.008310  139   \n",
       "4  0.0   ...    0.0  0.166667  0.0  0.0  0.658915  0.716377    0.011080  138   \n",
       "5  0.0   ...    0.0  0.333333  0.0  0.0  0.596899  0.624827    0.013850  137   \n",
       "6  0.0   ...    0.0  0.250000  0.0  0.0  0.550388  0.691798    0.016620  136   \n",
       "7  0.0   ...    0.0  0.416667  0.0  0.0  0.705426  0.591273    0.019391  135   \n",
       "8  0.0   ...    0.0  0.250000  0.0  0.0  0.744186  0.770367    0.022161  134   \n",
       "9  0.0   ...    0.0  0.250000  0.0  0.0  0.565891  0.673571    0.024931  133   \n",
       "\n",
       "   label1  label2  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       0       0  \n",
       "4       0       0  \n",
       "5       0       0  \n",
       "6       0       0  \n",
       "7       0       0  \n",
       "8       0       0  \n",
       "9       0       0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_pickle(SHARE_ROOT + TEST_DATA)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to recreate the feature engineering (creating the sequence features) just as we did in the model building step. We will do this within the webservice so that the service will take the raw data, and return a scored result predicting probability of failure at 30 days (`label1`). \n",
    "\n",
    "For scoreing, the model does not know the true labels, so we create a score_df without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the feature columns \n",
    "# Sequence help order the observations in \"time\"\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "# key columns group the machines\n",
    "key_cols = ['id', 'cycle']\n",
    "\n",
    "# Labels are what we're predicting.\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "# The scoreing data should not have labels... if we knew the label, \n",
    "# we wouldn'y need to predict.\n",
    "score_df = test_df.drop(label_cols, axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test init() and run() functions to read from the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web service requires two functions, an `init()` function that will load the model, and a `run()` function that will score a data set. We create the functions in this notebook for testing and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "    global loaded_model\n",
    "    \n",
    "    # load json and create model\n",
    "    with open(SHARE_ROOT + LSTM_MODEL, 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(os.path.join(SHARE_ROOT, MODEL_WEIGHTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(score_input): \n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "\n",
    "    input_features = score_input.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_input[score_input['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                 for id in score_input['id'].unique() if len(score_input[score_input['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        pred = prediction.tolist()\n",
    "        return(json.dumps(pred))\n",
    "    except Exception as e:\n",
    "        return(str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100]\n"
     ]
    }
   ],
   "source": [
    "print(score_df.id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The teset would be to first `initialize` the webservice, then send the entire scoring data set into the model. We expect to get 1 probability for each machine prediction. Since the `score_df` has 100 machines, we expect 100 probabilities back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0028877859003841877], [0.001371121034026146], [0.002124306047335267], [0.00152556411921978], [0.00100955949164927], [0.0015838078688830137], [0.0014917494263499975], [0.0014009204460307956], [0.0015001412248238921], [0.0010851335246115923], [0.0013480393681675196], [0.0009001982980407774], [0.0010240692645311356], [0.005088249687105417], [0.13305875658988953], [0.0012116295984014869], [0.8878982663154602], [0.0012445084284991026], [0.000758117123041302], [0.8709540367126465], [0.0008619278087280691], [0.002006620168685913], [0.0012240123469382524], [0.0015637396136298776], [0.0015616175951436162], [0.9830930233001709], [0.004435935523360968], [0.0012643851805478334], [0.9779555201530457], [0.9870022535324097], [0.6137491464614868], [0.1641593873500824], [0.0031632520258426666], [0.2553907036781311], [0.4884549379348755], [0.9526797533035278], [0.0021306502167135477], [0.0015662438236176968], [0.002298131585121155], [0.013850127346813679], [0.0015890055801719427], [0.0014705922221764922], [0.5502294301986694], [0.001176804187707603], [0.0018632253631949425], [0.08545172959566116], [0.049792055040597916], [0.000738465809263289], [0.001036008819937706], [0.9457030296325684], [0.0016720097046345472], [0.007476828992366791], [0.0007350374944508076], [0.001722258166410029], [0.18618370592594147], [0.00680731562897563], [0.0039924150332808495], [0.07743608206510544], [0.0006150148110464215], [0.8892070651054382], [0.0010136232012882829], [0.9845160245895386], [0.001278780517168343], [0.001732781296595931], [0.0009987226221710443], [0.0027042506262660027], [0.0009589741239324212], [0.0014225561171770096], [0.0013221639674156904], [0.986062228679657], [0.03649507462978363], [0.0005319581832736731], [0.001901011448353529], [0.0017288547242060304], [0.9797674417495728], [0.9610698819160461], [0.001062777591869235], [0.0027207264211028814], [0.0011795834871008992], [0.0007693300140090287], [0.0013083171797916293], [0.0011510316981002688], [0.11873245984315872], [0.4263748526573181], [0.2784743905067444], [0.008556141518056393], [0.0033222066704183817], [0.0011626164196059108], [0.0007733930251561105], [0.001234408700838685], [0.0024502689484506845], [0.0006520666065625846], [0.26249122619628906]]\n"
     ]
    }
   ],
   "source": [
    "init()\n",
    "\n",
    "prb=run(score_df)\n",
    "print(prb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we get 93, because 7 of the machines do not have the full 50 records available for scoring. If we send a machine with fewer than 50 records we get the following back: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error when checking : expected lstm_1_input to have 3 dimensions, but got array with shape (0, 1)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_df=score_df.loc[score_df['id'] == 1]\n",
    "\n",
    "print(tst_df.shape)\n",
    "\n",
    "# Because \n",
    "run(tst_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we send a complete data set, like machineID == 3, we get a probability back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 27)\n",
      "[[0.0028877872973680496]]\n"
     ]
    }
   ],
   "source": [
    "tst_df=score_df.loc[score_df['id'] == 3]\n",
    "\n",
    "print(tst_df.shape)\n",
    "\n",
    "# Because \n",
    "ans=run(tst_df)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model assets\n",
    "\n",
    "Next we persist the assets we have created to disk for use in operationalization. First we need to define the schema so the webservice knows what the data will look like as it comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"score_input\": SampleDefinition(DataTypes.PANDAS, score_df)}\n",
    "\n",
    "json_schema = generate_schema(run_func=run, inputs=inputs, filepath=SCHEMA_FILE)\n",
    "\n",
    "# save the schema file for deployment\n",
    "out = json.dumps(json_schema)\n",
    "with open(SHARE_ROOT + SCHEMA_FILE, 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conda dependencies are defined in this `webservices_conda.yaml` file. This will be used to tell the webservice server what python packages are required to run this web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /azureml-share//webservices_conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SHARE_ROOT}/webservices_conda.yaml\n",
    "\n",
    "# Conda environment specification. The dependencies defined in this file will\n",
    "# be automatically provisioned for managed runs. These include runs against\n",
    "# the localdocker, remotedocker, and cluster compute targets.\n",
    "\n",
    "# Note that this file is NOT used to automatically manage dependencies for the\n",
    "# local compute target. To provision these dependencies locally, run:\n",
    "# conda env update --file conda_dependencies.yml\n",
    "\n",
    "# Details about the Conda environment file format:\n",
    "# https://conda.io/docs/using/envs.html#create-environment-file-by-hand\n",
    "\n",
    "# For managing Spark packages and configuration, see spark_dependencies.yml.\n",
    "\n",
    "name: project_environment\n",
    "channels:\n",
    "- conda-forge\n",
    "- defaults\n",
    "dependencies:\n",
    "  - python=3.5.2\n",
    "  - pip:\n",
    "    - azure-common==1.1.8\n",
    "    - azure-storage==0.36.0\n",
    "    - numpy==1.14.0 \n",
    "    - sklearn\n",
    "    - keras\n",
    "    - tensorflow\n",
    "    - h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score file is python code defining what the web service will do. It includes both the `init()` and `run()` files defined earlier. These should be nearly identical to the previous defined versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /azureml-share//lstmscore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SHARE_ROOT}/lstmscore.py\n",
    "\n",
    "# import the libraries\n",
    "import keras\n",
    "import tensorflow\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "    global loaded_model\n",
    "    \n",
    "    # load json and create model\n",
    "    with open('modellstm.json', 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"modellstm.h5\")\n",
    "\n",
    "def run(score_input):\n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "\n",
    "    input_features = score_input.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_input[score_input['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                 for id in score_input['id'].unique() if len(score_input[score_input['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        \n",
    "        pred = prediction.tolist()\n",
    "        return(json.dumps(pred))\n",
    "    except Exception as e:\n",
    "        return(str(e))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    run(\"{\\\"score_df\\\": [{\\\"s20\\\": 0.5581395348837184, \\\"s10\\\": 0.0, \\\"s2\\\": 0.5451807228915584, \\\"s21\\\": 0.6618337475835432, \\\"s9\\\": 0.12761374854168395, \\\"s19\\\": 0.0, \\\"s3\\\": 0.31066056245912677, \\\"cycle\\\": 1, \\\"s15\\\": 0.3089649865332831, \\\"s4\\\": 0.2694125590817009, \\\"s1\\\": 0.0, \\\"s11\\\": 0.2083333333333357, \\\"cycle_norm\\\": 0.0, \\\"s13\\\": 0.2205882352941444, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.2121212121210192, \\\"s14\\\": 0.1321601816492901, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.75, \\\"setting1\\\": 0.632183908045977, \\\"s12\\\": 0.6460554371002161, \\\"s17\\\": 0.3333333333333357, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.6521739130434696}, {\\\"s20\\\": 0.6821705426356601, \\\"s10\\\": 0.0, \\\"s2\\\": 0.15060240963856586, \\\"s21\\\": 0.6868268434134208, \\\"s9\\\": 0.14668401687158195, \\\"s19\\\": 0.0, \\\"s3\\\": 0.37955090473076325, \\\"cycle\\\": 2, \\\"s15\\\": 0.21315890727203168, \\\"s4\\\": 0.2223160027008788, \\\"s1\\\": 0.0, \\\"s11\\\": 0.38690476190476275, \\\"cycle_norm\\\": 0.002770083102493075, \\\"s13\\\": 0.26470588235270043, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.16666666666696983, \\\"s14\\\": 0.20476829394158358, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.25, \\\"setting1\\\": 0.3448275862068965, \\\"s12\\\": 0.7398720682302695, \\\"s17\\\": 0.4166666666666714, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.8051529790660226}, {\\\"s20\\\": 0.7286821705426334, \\\"s10\\\": 0.0, \\\"s2\\\": 0.3765060240963862, \\\"s21\\\": 0.7213476940071786, \\\"s9\\\": 0.15808130664991182, \\\"s19\\\": 0.0, \\\"s3\\\": 0.34663178548071016, \\\"cycle\\\": 3, \\\"s15\\\": 0.4586379376683354, \\\"s4\\\": 0.3222484807562438, \\\"s1\\\": 0.0, \\\"s11\\\": 0.38690476190476275, \\\"cycle_norm\\\": 0.0055401662049861505, \\\"s13\\\": 0.2205882352941444, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.22727272727297532, \\\"s14\\\": 0.15564041696769948, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.5833333333333334, \\\"setting1\\\": 0.5172413793103449, \\\"s12\\\": 0.6993603411513902, \\\"s17\\\": 0.4166666666666714, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.6859903381642596}]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include a python test file `test_service.py` to test the web service. Sending the 50 records to the web service results in a command line string that is longer than currently supported. So this file will read the test_data file, and break it into chunks by machine ID for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /azureml-share//test_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SHARE_ROOT}/test_service.py\n",
    "\n",
    "import urllib\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# The URL will need to be editted after service create.\n",
    "url = 'http://127.0.0.1:32773/score'\n",
    "\n",
    "## Sequence length will need to match the training sequence length from\n",
    "## 2_model_building_and_evaluation.ipynb\n",
    "sequence_length = 50\n",
    "\n",
    "# We'll read in this data to test the service\n",
    "test_df = pd.read_pickle('PM_test_files.pkl')\n",
    "\n",
    "# Labels are what we're predicting.\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "# The scoreing data should not have labels... if we knew the label, \n",
    "# we wouldn't need to predict.\n",
    "score_df = test_df.drop(label_cols, axis = 1)\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# Now get the machine numbers, for each machine get the \n",
    "# prediction for the label timepoint\n",
    "machineID = score_df['id'].unique()\n",
    "\n",
    "for ind in machineID:\n",
    "    \n",
    "    try:\n",
    "        body = score_df[score_df.id==ind]\n",
    "        print('ID {}: size {}'.format(ind, body.shape))\n",
    "        if body.shape[0] < sequence_length : \n",
    "            print(\"Skipping machineID {} as we need {} records to score and only have {} records.\".format(ind, sequence_length, body.shape[0]))\n",
    "            continue\n",
    "        print('ID {}: {} \\t {}'.format(ind, body.shape, body.tail(sequence_length+ 10).shape))\n",
    "        body = \"{\\\"score_input\\\": \" + \\\n",
    "                    body.tail(sequence_length+10).to_json(orient=\"records\") +\\\n",
    "                    \"}\"\n",
    "        \n",
    "        req = urllib.request.Request(url, str.encode(body), headers) \n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            the_page = response.read()\n",
    "            print('ID {}: {}'.format(ind,the_page))\n",
    "        \n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code {}: \\n{}\".format(error, error.read))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(error.reason)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging\n",
    "\n",
    "To move the model artifacts around, we'll zip them into one file. We can then retreive this file from the persistance shared folder on your DSVM.\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/azureml-share//PM_train_files.pkl': No such file or directory\n",
      "PM_test_files.pkl  modellstm.h5    service_schema.json\twebservices_conda.yaml\n",
      "lstmscore.py\t   modellstm.json  test_service.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.storage.blob.models.ResourceProperties at 0x7fcf3a2c9c18>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compress the operationalization assets for easy blob storage transfer\n",
    "# We can remove the persisted data files.\n",
    "# !rm {SHARE_ROOT}/PM*.pkl\n",
    "!rm {SHARE_ROOT}/PM_train_files.pkl\n",
    "!ls {SHARE_ROOT}\n",
    "\n",
    "MODEL_O16N = shutil.make_archive('LSTM_o16n', 'zip', SHARE_ROOT)\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(MODEL_CONTAINER,\n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# Transfer the compressed operationalization assets into the blob container.\n",
    "az_blob_service.create_blob_from_path(MODEL_CONTAINER, \"LSTM_o16n.zip\", str(MODEL_O16N)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "Once the assets are stored, we can download them into a deployment compute context for operationalization on an Azure web service. For this scenario, we will deploy this on our local docker container context.\n",
    "\n",
    "We demonstrate how to setup this web service this through a CLI window opened in the AML Workbench application. \n",
    "\n",
    "## Download the model\n",
    "\n",
    "To download the model we've saved, follow these instructions on a local computer.\n",
    "\n",
    " - Open the Azure Portal\n",
    " - In the left hand pane, click on All resources\n",
    " - Search for the storage account using the name you provided earlier in this notebook.\n",
    " - Choose the storage account from search result list, this will open the storage account panel.\n",
    " - On the storage account panel, choose Blobs\n",
    " - On the Blobs panel choose the container `pmlstmmodel`\n",
    " - Select the file `LSTM_o16n.zip` and on the properties pane for that blob, choose download.\n",
    "\n",
    "Once downloaded, unzip the file into the directory of your choosing. The zip file contains three deployment assets:\n",
    "\n",
    "- the `lstmscore.py` file which contains functionst to do the model scoring\n",
    "- the `modellstm.json` model definition file\n",
    "- the `modellstm.h5` model weights file\n",
    "- the `service_schema.json` which defines the input data schema\n",
    "\n",
    "Additionally, because we are using both `keras` and `tensorflow` in this deployment, we will need to copy the `conda_dependencies.yaml` file from the `<project>\\aml_config` folder into this deployment directory. \n",
    "\n",
    "## Create a model management endpoint \n",
    "\n",
    "Create a modelmanagement under your account. We will call this `pdmmodelmanagement`. The remaining defaults are acceptable.\n",
    "\n",
    "`az ml account modelmanagement create --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "If you get a `ResourceGroupNotFound` error, you may need to set the correct subscription. This is typically only an issue if your Azure login connects to multiple subscritpions. \n",
    "\n",
    "`az account set -s '<subscription name>'`\n",
    "\n",
    "You can find the `subscription name` or `subscription id` through the (https://portal.azure.com) under the resource group you'd like to use.\n",
    "\n",
    "## Check environment settings\n",
    "\n",
    "Show what environment is currently active:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "If nothing is set, we setup the environment with the existing model management context first: \n",
    "\n",
    "` az ml env setup --location <ACCOUNT_REGION> --resource-group <RESOURCE_GROUP> --name pdmmodelmanagement`\n",
    "\n",
    "using the same `<ACCOUNT_REGION>` and `<RESOURCE_GROUP>` in the previous section. Then set the current environment:\n",
    "\n",
    "`az ml env set --resource-group <RESOURCE_GROUP> --cluster-name pdmmodelmanagement`\n",
    "\n",
    "Check that the environment is now set:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "## Deploy a web service \n",
    "\n",
    "These commands assume the current directory contains the webservice assets we created in throughout the notebooks in this scenario (`lstmscore.py`, `modellstm.json`, `modellstm.h5` and `service_schema.json`). Change to the directory where the zip file was unpacked. \n",
    "\n",
    "The command to create a web service (`<SERVICE_ID>`) with these operationalization assets in the current directory is:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f <filename> -r <TARGET_RUNTIME> -m <MODEL_FILE> -s <SCHEMA_FILE> -n <SERVICE_ID> --cpu 0.1\n",
    "`\n",
    "\n",
    "The default cluster has only 2 nodes with 2 cores each. Some cores are taken for system components. AMLWorkbench asks for 1 core per service. To deploy multiple services into this cluster, we specify the cpu requirement in the service create command as (--cpu 0.1) to request 10% of a core. \n",
    "\n",
    "For this example, we will call our webservice `lstmwebservice`. This `SERVICE_ID` must be all lowercase, with no spaces:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f lstmscore.py -r python -m modellstm.json -m modellstm.h5 -s service_schema.json -c webservices_conda.yaml --cpu 0.1 -n lstmwebservice\n",
    "`\n",
    "\n",
    "This command will take some time to execute. \n",
    "\n",
    "## Test your deployment.\n",
    "\n",
    "Once complete, the `az ml service create` command returns sample usage commands to test the service for both PowerShell and the cmd prompt. We can test this deployment by executing these commands from the command line. \n",
    "\n",
    "```\n",
    "> az ml service usage realtime -i lstmwebservice\n",
    "Scoring URL:\n",
    "    http://127.0.0.1:32770/score\n",
    "\n",
    "Headers:\n",
    "    Content-Type: application/json\n",
    "\n",
    "Swagger URL:\n",
    "    http://127.0.0.1:32770/swagger.json\n",
    "\n",
    "Sample CLI command:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Conclusion\n",
    "\n",
    "Working through all of these notebooks, we have completed:\n",
    "\n",
    " * Data aquisition in `Code/1_data_aquisition.ipynb` notebook.\n",
    " * Time series feature engineering and failure labeling to predict component failures within a 7 day window in the `Code/2_feature_engineering.ipynb` notebook.\n",
    " * Model building and evaluation in the `Code/3_model_building.ipynb` notebook.\n",
    " * Deployment asset generation and model deployment in the `Code/4_operationalization.ipynb` notebook.\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "LSTM DSVM",
   "language": "python",
   "name": "lstm_dsvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
